{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7a63b1-12e4-4719-b3da-9943ac7de9ac",
   "metadata": {},
   "source": [
    "# Dask - Exploratory Data Analysis\n",
    "\n",
    "In this courselet, we will explore the use of Dask as a Data Science tool when working with Big Data. By the end of this courselet, you would be able to:\n",
    "\n",
    "- Identify Sparks main objects\n",
    "- Identify some techniques to optimize Dask's performance.\n",
    "\n",
    "## What is Dask?\n",
    "\n",
    "[Dask](https://www.dask.org/) is an open-source Python library designed for parallel computing. It is considered to be an optimal framework to scale python projects by allowing the integration of distributed clusters. \n",
    "\n",
    "## Advantages of Dask\n",
    "\n",
    "- Purely pythonic: Dask is completely written in Python, and as such, it facilitates the interaction with other popular data science and machine learning libraries like Numpy, Pandas, scikit-learn, PyTorch, TensorFlow, Keras, among others.\n",
    "- User friendly: Given that dask is purely pythonic, its adoption, maintenance, development and debugging can be considered more user friendly compared to Spark.\n",
    "\n",
    "## Disadvantages of Dask\n",
    "\n",
    "- Large data limitations: Although Dask is a framework designed to deal with large datasets by relying in distributed and parallel computing, it might not be the best solution for cases of very large data (let's say TB's of serial data or streaming data). In those cases, Spark can be considered a more appropiate solution.\n",
    "- Limited fault tolerance: Although Dask offers some fault tolerance mechanisms within its design, these are not as complex nor effective as the ones offered by Spark. \n",
    "\n",
    "## Connecting to our cluster\n",
    "\n",
    "The first thing that we are going to do, is to connect to our Dask cluster. We are going to create a [Client](https://distributed.dask.org/en/latest/client.html) instance to connect to the scheduler and be able to send jobs to the workers. \n",
    "\n",
    "**NOTE:** For local testing and practice, review the documentation to set up a [LocalCluster](https://docs.dask.org/en/stable/deploying-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3d8b6-d5c4-46e9-a10f-c7a44111c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "client = Client(\"tcp://dask-scheduler:8786\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74fdf77-773f-46ce-bb5a-1c911b1324d3",
   "metadata": {},
   "source": [
    "## Task Graph\n",
    "\n",
    "A main concept to understand Dask is the concept of [Task Graph](https://docs.dask.org/en/latest/graphs.html). A Task Graph is a representation of the execution plan that Dask defines for the requested jobs. In this plan, a node represents a task, while the edges represent the dependencies between tasks. \n",
    "\n",
    "Similar to Spark, Dask works under a [Lazy Evaluation](https://saturncloud.io/blog/a-data-scientist-s-guide-to-lazy-evaluation-with-dask/) principle, which means that we can define as many execution tasks as desired within an execution plan, but non of them is executed until explicitly told to do so. This allows Dask to find an optimal execution plan by parallelizing the computation and avoiding reduntant processes. \n",
    "\n",
    "## Dask Array\n",
    "\n",
    "The first data structure that we are going to work with, is the [Dask Array](https://docs.dask.org/en/latest/array.html) A Dask Array is essentially a collection of Numpy arrays distributed among the cores of the system, to allow for parallelization. This distribution and coordination is organized through the use of graphs. \n",
    "\n",
    "When defining a new array, a key concept to keep in mind is the concept of [Chunks](https://docs.dask.org/en/latest/array-chunks.html) A chunk represents each of the different arrays or pieces that compose the whole Dask Array and that are distributed along the cores. [The choice of the right number of chunks](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes) entirely depends on the algorithm that we are going to execute and the size of our data. \n",
    "\n",
    "Now that we have covered a couple of relevant concepts in Dask, let's start running a first example. In the following lines of code, we are going to do the following:\n",
    "\n",
    "- Define a simple [Dask Array](https://docs.dask.org/en/latest/array.html)\n",
    "- Define the calculation of the sin of each element in the array\n",
    "- Define the calculation of the mean of all the elements within the array\n",
    "- Visualize the Task Graph to get a better understanding on how the job is executed\n",
    "- Compute the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c898878a-f70e-4bc7-a0a8-43d9b88972ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "rng = da.random.default_rng()\n",
    "x = rng.random((4, 4), chunks=(2, 2)) # We create a 4x4 array, splitted into chunks of size 2x2. Therefore, we will have 4 chunks (4x4/2x2 = 4)\n",
    "y = da.sin(x)\n",
    "mean = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366ef47-cc3a-4051-9e77-3fc4c541f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's look at the Task Graph\n",
    "\n",
    "mean.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084f231-c5ce-4989-9088-4276f27d60d7",
   "metadata": {},
   "source": [
    "As we see, the graph is representing the execution plan, in which we have 4 chunks of random values from our array. Each chunk is then passed to calculate the sin for each value, then the mean is calculated for each chunk and finally means are aggregated to return the final result.\n",
    "\n",
    "Now that we have our plan, we can execute it through the *.compute()* method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647c4ad-a4bf-41cf-a6b2-59db0b1b57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d71dc19-3a66-4783-b338-649efaa7c47b",
   "metadata": {},
   "source": [
    "## Custom Delayed Plans\n",
    "\n",
    "Dask possesses the flexibility of allowing us to define our own *lazy algorithm* by making use of the [delayed](https://docs.dask.org/en/stable/delayed.html) interface. This is mostly helpful for cases in which parallelization is achievable, but the structure of our data cannot be abstracted as an array or dataframe. The following example taken from the official documentation exemplifies a case clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ceeb8-5ca6-40ea-a128-278d0a703595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have the following functions and loop:\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    return x * 2\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "data = [1, 2, 3]\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = inc(x)\n",
    "    b = double(x)\n",
    "    c = add(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total = sum(output)\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a416ef-c463-4f5e-be33-a0f5c87f05f7",
   "metadata": {},
   "source": [
    "The previous code was eagerly executed. However, there was certainly room for an execution plan that could parallelize the tasks. For example, instead of actually looping through the elements of the list and performing the computations sequentially, a plan could take each element of the list and perform both the addition and the doubling for all elements in parallel, before adding the pairs and finally summing up all of the values.\n",
    "\n",
    "This is achievable through the [delayed](https://tutorial.dask.org/03_dask.delayed.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1db1f1-2e2b-4a4e-b9dd-1b84478f0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for x in data:\n",
    "    a = dask.delayed(inc)(x)\n",
    "    b = dask.delayed(double)(x)\n",
    "    c = dask.delayed(add)(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total_delayed = dask.delayed(sum)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4596e2-4647-4e37-8b33-f137ef837a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our graph\n",
    "total_delayed.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46444d59-5a32-4b4d-af79-fe297265a484",
   "metadata": {},
   "source": [
    "Now the initial code is represented as a graph. We can now perform the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e181e-cc5b-4d64-86d3-eb565e9055d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delayed.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89f6e3-d31a-40a1-a512-df17b218dc79",
   "metadata": {},
   "source": [
    "We can also pre-define our functions to be executed lazily through the *@delayed* decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd81efb-f006-4cb0-bbe6-7e2338055585",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def inc_del(x):\n",
    "    return x + 1\n",
    "\n",
    "@dask.delayed\n",
    "def double_del(x):\n",
    "    return x * 2\n",
    "\n",
    "@dask.delayed\n",
    "def add_del(x, y):\n",
    "    return x + y\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = inc_del(x)\n",
    "    b = double_del(x)\n",
    "    c = add_del(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total_dec = dask.delayed(sum)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b517a3-704c-4be7-8258-80a2ddfbf4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dec.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f39ec-7c8e-4d5b-86b4-4a3b087824d9",
   "metadata": {},
   "source": [
    "## The Dask DataFrame\n",
    "\n",
    "The main object to work with a dataset through Dask is the [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html), which is essentially a collection of multiple Pandas DataFrames parallelized across the cluster. \n",
    "\n",
    "To begin our exploratory data analysis, we'll start by loading the data and creating our dataframe. For this courselet, we are using taxi trips reported to the City of Chicago in 2020. This data is publicly available through the [Chicago Data Portal](https://data.cityofchicago.org/en/Transportation/Taxi-Trips-2020/r2u4-wwk3/about_data). \n",
    "\n",
    "For this example, we are using the data in [Parquet format](https://parquet.apache.org/) This open-source format offers efficient data storage and compression, as well as better data retrieval than traditional CSV. Therefore, given the data is available in this format, we take advantage of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e645dc-aea3-4120-97ee-3eab6eb3e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the df\n",
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dd.read_parquet(\"data/chicago-taxi-2020.parquet\", assume_missing=True)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd8b2fa-b3ed-4380-9d6b-1078e51fb381",
   "metadata": {},
   "source": [
    "We will now start performing some descriptive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428e1a2-10da-4475-aea3-56f93b93ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting non-empty rows per column\n",
    "non_empty_count = ddf.count()\n",
    "non_empty_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef91026-19ba-48fe-8177-376762a018c0",
   "metadata": {},
   "source": [
    "Why we cannot see the results? As we stated at the beginning, Dask will not perform any computation until explicitly calling the *.compute()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ecb8a3-cc10-42c1-be37-e61e4162af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will get the actual result\n",
    "non_empty_count.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d4f5ad-29b9-4917-83f1-a20c0698a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missingness rate per column\n",
    "ddf.isnull().mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f788756-e75e-4cec-9823-b5908c53e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the top 10 rows of 2 columns\n",
    "ddf[['Trip ID', 'Trip Total']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8066296-7873-4af0-a261-6f96eb21adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of a column\n",
    "ddf['Trip Total'].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf4026-cbbb-47eb-9e8e-0bd647490d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max value of a column\n",
    "ddf['Trip Total'].max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62850985-bde5-4495-adc9-d69903f5f60d",
   "metadata": {},
   "source": [
    "## Visualizing Data\n",
    "\n",
    "Given that Dask is a completely pythonic framework, data visualization is supported through popular libraries such as matplotlib and seaborn. Let's do a quick visualization example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c61d8-d2d4-4cb5-9b83-cfb57b1125a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's say we want to visualize the avg trip total cost by payment type\n",
    "viz_ddf = ddf.dropna(subset=['Trip Total', 'Payment Type']) # We subset the data\n",
    "avg_trip_total_by_payment_type = viz_ddf.groupby('Payment Type')['Trip Total'].mean().compute() # We create a small pd.df with the information grouped\n",
    "\n",
    "# Now we visualize as we would normally do\n",
    "avg_trip_total_by_payment_type.plot(kind='bar', color='skyblue')\n",
    "plt.xlabel('Payment Type')\n",
    "plt.ylabel('Average Trip Total (USD)')\n",
    "plt.title('Average Trip Total by Payment Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aef4a0-d82a-4d82-ac03-4c2894fe0eca",
   "metadata": {},
   "source": [
    "More on data visualization can be found in the [FOUNT Courselets library](https://voices.uchicago.edu/fount/resources/fount-courselets/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07127a9c-02db-4433-93b8-15c279e4da61",
   "metadata": {},
   "source": [
    "## Improving Dataframes performance\n",
    "\n",
    "When working with dataframes, Dask suggests a series of techniques that can help us improve the performance. Let's take a look at some of them.\n",
    "\n",
    "**Repartition** \n",
    "\n",
    "When we load a dataframe, Dask will evaluate the resources, datasize and type and choose an n number to partition and distribute our dataframe. However, this n number of partitions might not be the most optimal, and so, we can perform a repartition to our data to potentially improve performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e79751-e292-42b8-b572-a6e878dfb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's take a look at the current number of partitions\n",
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c63dc-2a68-416d-9e59-5eb3cbc90c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_time(file_path, num_partitions):\n",
    "    # Read the data and repartition\n",
    "    ddf = dd.read_parquet(file_path, assume_missing=True)\n",
    "    ddf = ddf.repartition(npartitions=num_partitions)\n",
    "    \n",
    "    # Measure computation time\n",
    "    start_time = time.time()\n",
    "    result = ddf['Trip Total'].mean().compute()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "file_path = \"data/chicago-taxi-2020.parquet\"\n",
    "\n",
    "# Different numbers of partitions to test\n",
    "num_partitions_list = [1, 2, 4, 8, 16]\n",
    "\n",
    "times = []\n",
    "for num_partitions in num_partitions_list:\n",
    "    time_taken = measure_time(file_path, num_partitions)\n",
    "    times.append(time_taken)\n",
    "\n",
    "# Plotting the results\n",
    "plt.plot(num_partitions_list, times, marker='o')\n",
    "plt.xlabel('Number of Partitions')\n",
    "plt.ylabel('Time taken (seconds)')\n",
    "plt.title('Impact of Number of Partitions on Computation Time')\n",
    "plt.xticks(num_partitions_list)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Identify the Number of Partitions with the Lowest Time\n",
    "min_time = min(times)\n",
    "optimal_num_partitions = num_partitions_list[times.index(min_time)]\n",
    "print(f\"The optimal number of partitions is {optimal_num_partitions} with a time of {min_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beee68d-56d7-4e7a-a40c-5959d13ecce5",
   "metadata": {},
   "source": [
    "**Indexing a column**\n",
    "\n",
    "Indexing our dataframe by a column through the use of the [*.set_index()*](https://docs.dask.org/en/latest/generated/dask.dataframe.DataFrame.set_index.html) method can help us perform faster and more efficient queries. In this case, we can take advantage of one of the two timestamps and create an index from it. This would be a smart strategy as this data is very likely to be queried constantly incorporating temporality conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c53b0-5223-4954-ac1f-511e8ae88f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by generating a new column in time format. We make this choice instead of overwriting the original column\n",
    "ddf['timestamp_index'] = dd.to_datetime(ddf['Trip Start Timestamp']) \n",
    "\n",
    "# Now we make index our ddf using the new column\n",
    "ddf = ddf.set_index('timestamp_index', sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5006e-1715-4c53-b666-9660eec88155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can use our new index to subset our data\n",
    "january_ddf = ddf.loc['2020-01'] # This operation is much faster through an index than without it\n",
    "\n",
    "january_ddf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395081ec-cd35-4a99-b22d-73f06c976d2d",
   "metadata": {},
   "source": [
    "**Persisting Data**\n",
    "\n",
    "Another useful technique to improve our performance is to [persist](https://docs.dask.org/en/stable/api.html#dask.persist) in memory the recurrently used data. By doing this, we avoid the constant recomputation of the dataset, and our queries are executed atop of a persisted object, optimizing performance. According to Dask documentaion, the following would be a smart workflow:\n",
    "\n",
    "1. Load the data\n",
    "2. Filter (if needed) the data to a particular subset\n",
    "3. Set a smart index\n",
    "4. Persist the data in memory with the use of the *client.persist* method.\n",
    "\n",
    "We can do a example of this workflow by using the examples we have shown previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616a5e7-b381-444a-836d-3c2081a0a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading\n",
    "ddf = dd.read_parquet(\"data/chicago-taxi-2020.parquet\", assume_missing=True) \n",
    "\n",
    "# 2. Filtering - Let's say we only want to analyze the trips that were payed with Cash\n",
    "cash_ddf = ddf[ddf[\"Payment Type\"] == \"Cash\"]\n",
    "\n",
    "# 3. Indexing - We use our previous timestamp example\n",
    "cash_ddf['timestamp_index'] = dd.to_datetime(cash_ddf['Trip Start Timestamp']) \n",
    "cash_ddf = cash_ddf.set_index('timestamp_index', sorted=True)\n",
    "\n",
    "# 4. Persisting data\n",
    "cash_ddf = client.persist(cash_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fc453-56a4-4415-8bd1-2db32481ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can perform operations which should be faster\n",
    "cash_ddf['Trip Total'].mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447dccb3-4920-4591-804a-1ad695adcc6f",
   "metadata": {},
   "source": [
    "This section has been developed by following [official documentation on Best Practices](https://docs.dask.org/en/stable/dataframe-best-practices.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1884fc0-ac45-4b61-acd8-1ba1e9a40e78",
   "metadata": {},
   "source": [
    "## Bags\n",
    "\n",
    "The final type of Dask object that we are going to explore are [Dask Bags](https://docs.dask.org/en/stable/bag.html) Dask Bags are a special type of object that allows us to perform parallelized operations on Python objects that do not follow the structure of a Pandas Dataframe nor Numpy Array. Bags are particullarly useful when working with unstructured data like JSON files. Lets look at a example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6ed92-ca7c-4d48-acc2-2f67ae1407c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine that we had a json file that looked like the following unstructured data, representing a series of social media posts\n",
    "\n",
    "posts = [\n",
    "    {\"id\": 1, \"text\": \"Love using #dask for data processing!\", \"tags\": [\"data\", \"python\", \"dask\"], \"user\": {\"id\": 100, \"name\": \"John\"}},\n",
    "    {\"id\": 2, \"text\": \"Exploring the #python ecosystem is fascinating. #datascience\", \"user\": {\"id\": 101, \"name\": \"Jane\"}},\n",
    "    {\"id\": 3, \"text\": \"Check out my latest #blog on #machinelearning.\", \"tags\": [\"AI\", \"blog\", \"machinelearning\"], \"user\": {\"id\": 102, \"name\": \"Jack\"}},\n",
    "    # Some posts might not have hashtags or are missing other fields\n",
    "    {\"id\": 4, \"text\": \"Just another day training a #machinelearning model with #dask.\", \"user\": {\"id\": 103, \"name\": \"Jill\"}}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca207ab-4969-4ab3-b58d-af7f30de2caa",
   "metadata": {},
   "source": [
    "As you can observe, the *posts* data is not structured, as some posts come with tags and some other don't. Imagine an scenario in which we wanted to analyze thousands of posts from a social media site. We would certainly like to take advantage of parallelization to do this process. This is where Dask Bags can help us. Suppose that we wanted to count the frequency of hashtags among posts. The following code performs that operation by transforming our posts data into a bag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468e500-d4b1-4641-b1f1-08264dc66a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import re\n",
    "\n",
    "posts_bag = db.from_sequence(posts)\n",
    "\n",
    "# Function to extract hashtags from text\n",
    "def extract_hashtags(post):\n",
    "    hashtags = re.findall(r\"#(\\w+)\", post[\"text\"])\n",
    "    return hashtags\n",
    "\n",
    "# Function to filter posts with text field\n",
    "def filter_posts_with_text(post):\n",
    "    return \"text\" in post\n",
    "\n",
    "# Apply filter and map functions, then flatten the list of lists\n",
    "hashtags = posts_bag.filter(filter_posts_with_text).map(extract_hashtags).flatten()\n",
    "\n",
    "# Count occurrences of each hashtag\n",
    "hashtag_counts = hashtags.frequencies().compute()\n",
    "\n",
    "hashtag_counts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
